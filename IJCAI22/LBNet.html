<!DOCTYPE html>
<html>
<head>
<title>IJCAI22_LBNet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
<h3 align="center"><i><font size="5" face="Palatino Linotype">31st International Joint Conference on Artificial Intelligence (IJCAI) 2022</font></i></h3>

<table align="center">
<td align="center">
<h1>Lightweight Bimodal Network for Single-Image Super-Resolution <br> via Symmetric CNN and Recursive Transformer</h1>
<h3>
	<a href="https://guangweigao.github.io/" target="_blank"><font size="4"><b>Guangwei Gao</b></font></a><sup><font size="2">1†</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4"><b>Zhengxue Wang</b></font></a><sup><font size="2">1†</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://junchenglee.com/" target="_blank"><font size="4"><b>Juncheng Li</b></font></a><sup><font size="2">2*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4">Wenjie Li</font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4">Yi Yu</font><sup><font size="2">3</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://www.math.cuhk.edu.hk/~zeng/" target="_blank"><font size="4">Tieyong Zeng</font></a><sup><font size="2">2</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
</h3>

<sup><font size="2">1</font></sup>
<b><a><font size="4">Nanjing University of Posts and Telecommunications</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="4">The Chinese University of Hong Kong</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp; <br>
<sup><font size="2">3</font></sup>
<b><a><font size="4">National Institute of Informatics</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;

<br>
<br>&nbsp;
	<b><a><font size="4"> †Co-ﬁrst authors, *Corresponding author &nbsp;&nbsp;&nbsp;&nbsp; Contact us: {csggao,cvjunchengli}@gmail.com, wzx_0826@163.com</font></a></b>
<br>

</td>
</table>


<br><br>
<table align="center">
<tr>
	<td align="center"><embed src="LBNet-tradeoff.png" width="800"></td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="4" face="Palatino Linotype">Single-image super-resolution (SISR) has achieved significant breakthroughs with the development of deep learning. 
	However, these methods are difficult to be applied in real-world scenarios since they are inevitably accompanied by the problems of computational and memory costs caused by the complex operations. 
	To solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR. Specifically, an effective Symmetric CNN is designed for local feature extraction and coarse image reconstruction. 
	Meanwhile, we propose a Recursive Transformer to fully learn the long-term dependence of images thus the global information can be fully used to further refine texture details. 
	Studies show that the hybrid of CNN and Transformer can build a more efficient model. Extensive experiments have proved that our LBNet achieves more prominent performance than other state-of-the-art methods with a relatively low computational cost and memory consumption. 
	The code is available at <i>https://github.com/IVIPLab/LBNet</i>.
</font></p>

<!--
<br>
<h2><p><font size="6"><b>Motivation</b></font></p></h2>
<hr/>
<table align="center">
</table>
<p><font size="4" face="Palatino Linotype">As shown in the figure below, the inner areas of the boxes with the same color are similar to each other. Therefore, these similar image patches can be used as reference images for each other, so that the texture details of the certain patch can be restored with reference patches. Inspired by this, we introduce the Transformer into the SISR task since it has a strong feature expression ability to model such a long-term dependency in the image. In recent years, some Vision-Transformer have been proposed for computer vision tasks. However, these methods often occupy heavy GPU memory, which greatly limits their ﬂexibility and application scenarios. Moreover, these methods cannot be directly transferred to SISR since the image restoration task often take a larger resolution image as input, which will take up huge memory. To solve this, we aim to explore a more efﬁcient vision-Transformer for SISR.
</font></p>
<table align="center">
<tr>
	<td align="center"><img border=0 width=600 src="EP.png"></td>
</tr>
<tr>
<td align="center">Examples of similar patches in images. These similar patches can help restore details from each other.</td>
</tr>
</table>-->


<br>
<h2><p><font size="6"><b>LBNet</b></font></p></h2>
<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-network.png"></td>
</tr>
<br>	
<tr>
	<td align="center">The complete architecture of the proposed Lightweight Bimodal Network (LBNet).</td>
</tr>
</table>


<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-LFFM.png"></td>
</tr>
<br>
<tr>
	<td align="center">The architecture of the proposed Local Feature Fusion Module (LFFM) and Feature Refinement Dual-Attention Block (FRDAB).</td>
</tr>
</table>




<br>
<h2><p><font size="6"><b>PSNR/SSIM Results</b></font></p></h2>
<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-tableresult.png"></td>
</tr>
</table>




<br>
<h2><p><font size="6"><b>Visual Results</b></font></p></h2>

<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-visualresult1.png"></td>
</tr>
<br>
</table>


<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-visualresult2.png"></td>
</tr>
	<br>
</table>

<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-visualresult3.png"></td>
</tr>
	<br>
</table>


<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-visualresult4.png"></td>
</tr>
	<br>
</table>

<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="LBNet-visualresult5.png"></td>
</tr>
	<br>
</table>



<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>						
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: <a href="https://arxiv.org/abs/2112.08655" target="_blank">[ Paper ]</a></font>
		</td>
		</tr>
			
       <tr align="left">
		<td>
			<font size="4">Supp Material</font>
		</td>
		<td>
			<font size="4">: <a href="https://pan.baidu.com/s/1Li4pmzKyfhGZwi_MWjiqgQ" target="_blank">[ Supplementary Material(提取码:3ywm) ]</a></font>
		</td>
	    </tr>
					
		<tr align="left">
		<td>
			<font size="4">Source Code</font>
		</td>
		<td>
			<font size="4">: <a href="https://github.com/IVIPLab/LBNet" target="_blank">[ Code ]</a> </font>
		</td>
		</tr>	
			
			
		</table>
</div>

<!--
<h2><p><font size="6" color="black"><b>Statement</b></font></p></h2>
<hr/>
<font size="3">
The original title of this paper was "Efficient Transformer for Single Image Super-Resolution". </br>
The final version of this paper is "Transformer for Single Image Super-Resolution". 
</font>-->


<h2><p><font size="6" color="black"><b>BibTex</b></font></p></h2>
<hr/>
<font size="3">
@InProceedings{gao2022lightweight,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Gao, Guangwei and Wang, Zheengxue and Li, Juncheng and Li, Wenjie and Yu, Yi and Zeng, Tieyong},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {IJCAI},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2022}<br>
}
</font>


</body>

</html>
