<!DOCTYPE html>
<html>
<head>
<title>TMM22_FBSNet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
<h3 align="center"><i><font size="5" face="Palatino Linotype">IEEE Transactions on Multimedia (TMM) 2022</font></i></h3>

<table align="center">
<td align="center">
<h1>FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation</h1>
<h3>
	<a href="https://guangweigao.github.io/" target="_blank"><font size="4"><b>Guangwei Gao</b></font></a><sup><font size="2">1†</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4"><b>Guoan Xu</b></font></a><sup><font size="2">1†</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://junchenglee.com/" target="_blank"><font size="4"><b>Juncheng Li</b></font></a><sup><font size="2">2*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4">Yi Yu</font><sup><font size="2">3*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="4">Huimin Lu</font><sup><font size="2">4</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="http://www.patternrecognition.asia/jian/" target="_blank"><font size="4"><b>Jian Yang</b></font><sup><font size="2">5</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
</h3>

<sup><font size="2">1</font></sup>
<b><a><font size="4">Nanjing University of Posts and Telecommunications</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="4">The Chinese University of Hong Kong</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp; <br>
<sup><font size="2">3</font></sup>
<b><a><font size="4">National Institute of Informatics</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">4</font></sup>
<b><a><font size="4">Kyushu Institute of Technology</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">4</font></sup>
<b><a><font size="4">Nanjing University of Science and Technology</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;


<br>
<br>&nbsp;
	<b><a><font size="4"> †Co-ﬁrst authors, *Corresponding author &nbsp;&nbsp;&nbsp;&nbsp; Contact us: {csggao,cvjunchengli}@gmail.com, xga_njupt@163.com</font></a></b>
<br>

</td>
</table>


<br><br>
<table align="center">
<tr>
	<td align="center"><embed src="FBSNet-Tradeoff.png" width="900"></td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="4" face="Palatino Linotype">Real-time semantic segmentation, which can be visually understood as the pixel-level classification task on the input image, currently has broad application prospects, especially in the fast-developing fields of autonomous driving and drone navigation. However, the huge burden of calculation together with redundant parameters are still the obstacles to its technological development. 
	In this paper, we propose a Fast Bilateral Symmetrical Network (FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a symmetrical encoder-decoder structure with two branches, semantic information branch and spatial detail branch. The Semantic Information Branch (SIB) is the main branch with semantic architecture to acquire the contextual information of the input image and meanwhile acquire sufficient receptive field. 
	While the Spatial Detail Branch (SDB) is a shallow and simple network used to establish local dependencies of each pixel for preserving details, which is essential for restoring the original resolution during the decoding phase. Meanwhile, a Feature Aggregation Module (FAM) is designed to effectively combine the output of these two branches. Experimental results of Cityscapes and CamVid show that the proposed FBSNet can strike a good balance between accuracy and efficiency. 
	Specifically, it obtains 70.9\% and 68.9\% mIoU along with the inference speed of 90 fps and 120 fps on these two test datasets, respectively, with only 0.62 million parameters on a single RTX 2080Ti GPU. The code is available at https://github.com/IVIPLab/FBSNet.
</font></p>

<!--
<br>
<h2><p><font size="6"><b>Motivation</b></font></p></h2>
<hr/>
<table align="center">
</table>
<p><font size="4" face="Palatino Linotype">As shown in the figure below, the inner areas of the boxes with the same color are similar to each other. Therefore, these similar image patches can be used as reference images for each other, so that the texture details of the certain patch can be restored with reference patches. Inspired by this, we introduce the Transformer into the SISR task since it has a strong feature expression ability to model such a long-term dependency in the image. In recent years, some Vision-Transformer have been proposed for computer vision tasks. However, these methods often occupy heavy GPU memory, which greatly limits their ﬂexibility and application scenarios. Moreover, these methods cannot be directly transferred to SISR since the image restoration task often take a larger resolution image as input, which will take up huge memory. To solve this, we aim to explore a more efﬁcient vision-Transformer for SISR.
</font></p>
<table align="center">
<tr>
	<td align="center"><img border=0 width=600 src="EP.png"></td>
</tr>
<tr>
<td align="center">Examples of similar patches in images. These similar patches can help restore details from each other.</td>
</tr>
</table>-->

	
	
<br>
<h2><p><font size="6"><b>FBSNet</b></font></p></h2>
<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="FBSNet-Network.png"></td>
</tr>
<br>	
<tr>
	<td align="center">The complete architecture of our proposed Fast Bilateral Symmetrical Network (FBSNet).</td>
</tr>
</table>


<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="FBSNet-module.png"></td>
</tr>
<br>
<tr>
	<td align="center">Comparison of different residual modules. (a) SS-nbt module in LEDNet, (b) EAR module in MSCFNet, (c) Pyramid cascade module in ESPNet, and (d) is our proposed Bottleneck Residual Unit (BRU).</td>
</tr>
</table>


<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="FBSNet-architectural.png"></td>
</tr>
<br>
<tr>
	<td align="center">Detailed architectural configuration of Fast Bilateral Symmetrical Network (FBSNet).</td>
</tr>
</table>




<br>
<h2><p><font size="6"><b>Segmentation Results</b></font></p></h2>

<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=1000 src="FBSNet-cityscapes-visual.png"></td>
</tr>
<br>
<tr>
	<td align="center">Sample visual results on the Cityscapes validation set. From left to right: input samples, ground-truth references, segmentation outputs of the proposed FBSNet, LEDNet, DABNet, ERFNet, NDNet, and ENet. 
		The region in the yellow dotted box can intuitively highlight the superiority of our method over others.</td>
</tr>
</table>


<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=1000 src="FBSNet-cityscapes-table.png"></td>
</tr>
	<br>
<tr>
	<td align="center">Comparison with state-of-the-arts image semantic segmentation methods on the Cityscapes test dataset. Although our FBSNet did not achieve the best results on mIoU, the model achieves competitive results on the number of parameters, 
		FLOPs, and FPS. In general, FSBNet achieves the best balance between model performance, model size, and inference time.</td>
</tr>
</table>





<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>						
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: <a href="https://guangweigao.github.io/TMM22/FBSNet.pdf" target="_blank">[ Paper ]</a></font>
		</td>
		</tr>
			
					
		<tr align="left">
		<td>
			<font size="4">Source Code</font>
		</td>
		<td>
			<font size="4">: <a href="https://github.com/IVIPLab/FBSNet" target="_blank">[ Code ]</a> </font>
		</td>
		</tr>	
			
			
		</table>
</div>

<!--
<h2><p><font size="6" color="black"><b>Statement</b></font></p></h2>
<hr/>
<font size="3">
The original title of this paper was "Efficient Transformer for Single Image Super-Resolution". </br>
The final version of this paper is "Transformer for Single Image Super-Resolution". 
</font>-->


<h2><p><font size="6" color="black"><b>BibTex</b></font></p></h2>
<hr/>
<font size="3">
@InProceedings{gao2022fbsnet,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {FBSNet: A fast bilateral symmetrical network for real-time semantic segmentation},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Gao, Guangwei and Xu, Guoan and Li, Juncheng and Yu, Yi and Lu, Huimin and Yang, Jian},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {IEEE Transactions on Multimedia},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2022}<br>
}
</font>


</body>

</html>
